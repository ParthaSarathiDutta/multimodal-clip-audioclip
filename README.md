# ğŸ§ğŸ–¼ï¸ Multimodal Transformer with CLIP + AudioCLIP

This project demonstrates how to align vision and audio using pretrained models (CLIP and AudioCLIP). The system learns a shared embedding space where similar visual and audio inputs are close together.

## ğŸš€ Project Goals
- Learn joint embeddings from paired image-audio data
- Perform cross-modal retrieval: retrieve audio from image, and vice versa
- Explore multimodal scene classification using fused embeddings

## ğŸ’¡ Why This Matters
- ğŸ”¥ Multimodal learning is central to perception models at Meta AI, DeepMind, and FAIR
- ğŸ¯ Useful for tasks like video understanding, scene grounding, and AR/VR

## ğŸ“ Project Structure
```
multimodal-clip-audioclip/
â”œâ”€â”€ data/                       # Image-audio dataset (sample set included)
â”œâ”€â”€ models/                    # Fusion model (CLIP + AudioCLIP)
â”œâ”€â”€ scripts/                   # Training and evaluation scripts
â”œâ”€â”€ utils/                     # Dataloader and audio tools
â”œâ”€â”€ plots/                     # Retrieval heatmaps or metrics
â”œâ”€â”€ config.json                # Hyperparameters
â”œâ”€â”€ requirements.txt           # Python packages
â””â”€â”€ README.md
```

## ğŸ› ï¸ How to Run
```bash
pip install -r requirements.txt
python scripts/train.py
python scripts/eval_retrieval.py
```

## ğŸ“Š Outputs
- Retrieval accuracy: Top-1, Top-5
- Heatmaps: Audio â†’ Image similarity and Image â†’ Audio similarity

## ğŸ¤– Extensions
- Use ViT + wav2vec2.0 or BEATs instead of CLIP/AudioCLIP
- Add text modality (CLIP supports it)
- Use this for multimodal event detection in videos

## ğŸ“œ License
MIT â€” open to modify and use with attribution.
